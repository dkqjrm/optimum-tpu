{
    "llm_model_name_or_path": "/home/hyun/Qwen3-0.6B",
    "train_dataset": "huggingface:databricks/databricks-dolly-15k:train",
    "eval_dataset": "huggingface:databricks/databricks-dolly-15k:validation",
    "output_dir": "./logs/qwen_lora_training",
    "resume_from_checkpoint": null,
    "bf16": true,
    "max_grad_norm": 1.0,
    "max_steps": 2000,
    "per_device_train_batch_size": 32,
    "per_device_eval_batch_size": 16,
    "gradient_accumulation_steps": 1,
    "eval_steps": 300,
    "save_strategy": "no",
    "learning_rate": 0.0001,
    "weight_decay": 0.01,
    "adam_beta2": 0.95,
    "warmup_steps": 100,
    "lr_scheduler_type": "cosine",
    "logging_steps": 10,
    "report_to": "wandb",
    "run_name": "qwen_lora_tpu",
    "model_max_length": 2048,
    "dataloader_num_workers": 4,
    "dataloader_prefetch_factor": 4,
    "dataloader_drop_last": true,
    "dataloader_pin_memory": true,
    "dataloader_persistent_workers": true,
    "remove_unused_columns": false,
    "seed": 42,
    "dataset_text_field": "text",
    "max_length": 2048,
    "packing": false,
    "completion_only_loss": true,
    "lora_save_steps": 500,
    "lora_save_epochs": null
}